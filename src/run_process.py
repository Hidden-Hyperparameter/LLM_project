import os
import threading
import subprocess
import json

from utils.others import gen_10_pages,encode,filetype
from utils.ocr import OCR,TMP_DIR
from db.db import make_db_from_text,clear_cache
LOG_DIR = '/ssdshare/.it/files2.json'
IMMUTABLE = '/ssdshare/.it/ocr/'
OCR_LOG_DIR = IMMUTABLE + 'log2.txt'
query_lock = threading.Lock()
log_lock = threading.Lock()

def query_llm(texts:str,file:str,quiet=True):
    """
    Input: texts in first 10 pages
    Output: tags generated by LLM, formatted as a List
    """
    ##############################################################################
    #                  TODO: You need to complete the code here                  #
    ##############################################################################
    Full = "Give me some(about 10-20) tags based on the context below, without any other things!\n"+texts
    QUERY_TXT = os.path.join(TMP_DIR,f'Query_{encode(file)}.txt')
    QUERY_TAG = os.path.join(TMP_DIR,f'Query_{encode(file)}.tag')
    with open(QUERY_TXT, 'w') as f:
        f.write(Full)
    if not quiet:
        print('[QUERY_LLM] running llama3 70b')
    os.system(f"/share/ollama/ollama run llama3:70b < {QUERY_TXT} > {QUERY_TAG}")
    texts = []
    with open(QUERY_TAG, 'r') as f:
        lines = f.readlines()
        for line in lines:
            # check if it starts with a number
            if line[0].isdigit():
                items = line.split(' ')[1:]
                string = ''
                for item in items:
                    string += item + ' '
                texts.append(string.replace('\n', '').replace('*','').replace('#','').replace('\'','').replace('\"','')[:-1])
    ##############################################################################
    #                              END OF YOUR CODE                              #
    ##############################################################################
    return texts

def _log(file,tags):
    if not os.path.exists(LOG_DIR):
        with open(LOG_DIR,'w') as fw:
            fw.write(r'{}')
    with open(LOG_DIR,'r') as fr:
        dic = json.load(fr)
    if file in dic:
        print(f'[RUN_PROCESS][WARNING]:file {file} is already processed, so we overwrite.')
    dic[file]=tags
    with open(LOG_DIR,'w') as fw:
        json.dump(dic,fw,indent=4)


def full_nopdf(file:str,quiet,ocr_log_dir):
    print('FULL_NOPDF',file)
    file = os.path.abspath(file)
    texts = OCR(file,quiet=quiet)
    with query_lock:
        tags = query_llm(texts,file,quiet)
    with log_lock:
        _log(file,tags)
    if not os.path.exists(ocr_log_dir):
        with open(ocr_log_dir,'w') as f:
            f.write(f'\n{file}\n')
    else:
        with open(ocr_log_dir,'a') as f:
            f.write(f'\n{file}\n')
    make_db_from_text(file,texts,quiet=quiet)
    

def full_pdf(file:str,quiet):
    print('FULL_PDF',file)
    file = os.path.abspath(file)
    out_path = os.path.join(TMP_DIR,encode(file)+'_10pages.pdf')
    try:
        gen_10_pages(file,out_path=out_path)
    except BaseException as e:
        if not quiet:
            print('[RUN_PROCESS]: \033[31m[FATAL]\033[0m OCR EXCEPTION:',e)
        with query_lock:
            tags = query_llm(file,file,quiet)
        with log_lock:
            _log(file,tags)
        return 
    out_path = os.path.abspath(out_path)
    texts = OCR(out_path,quiet=quiet)
    os.remove(out_path)
    with query_lock:
        tags = query_llm(texts,file,quiet)
    with log_lock:
        _log(file,tags)
        
def make_db(file:str,ocr_log_dir:str,quiet):
    file = os.path.abspath(file)
    devnull = open(os.devnull,'w')
    if quiet:
        subprocess.Popen(['python','./ocr_process.py','--file','\"'+file+'\"','--log-dir','\"'+ocr_log_dir+'\"'],stderr=devnull,stdout=devnull)
    else:
        subprocess.Popen(['python','./ocr_process.py','--file','\"'+file+'\"','--log-dir','\"'+ocr_log_dir+'\"'])


NOT_READABLE = ['pdf','pptx','ppt','doc','docx','png','jpg','jpeg']

def clear_all(ocr_log_dir):
    """Clear all log files"""
    os.makedirs('/ssdshare/.it/ocr',exist_ok=True)
    clear_cache()
    with open(ocr_log_dir,'w') as fw:
        fw.write('')
    with open(LOG_DIR,'w') as fw:
        fw.write(r'{}')

def run(root_dir = './tests/data/hierarchical_data_simple',ocr_log_dir=OCR_LOG_DIR,quiet=True):
    """
    ## NOTICE: This function can only be run once!

    Run Full OCR and create tags, Chroma db concurrently given the root directory `root_dir`. 

    1. We first create all tags for each file. After that, the program stops.
    2. Then, the program create new processes for running OCR. These processes don't stop even after the main program ends. The logs are saved in ocr_log_dir.
    """
    # remove the ocr log, so it can only be run once.
    clear_all(ocr_log_dir)
    l = os.walk(root_dir)
    threads = []
    failures = []
    # first spread out threads to create tags
    for dirname,dirs,files in l:
        if not quiet:
            print(dirname,dirs,files)
        for file in files:
            if not quiet:   
                print(f'[RUN_PROCESS]:Starting Generating Tags on {file}')
            if filetype(file) in ['text','image']:
                # file is readable
                thread = threading.Thread(target=full_nopdf,args=(os.path.join(dirname,file),quiet,ocr_log_dir))
            else:
                full_path = os.path.join(dirname,file)
                thread = threading.Thread(target=full_pdf,args=(full_path,quiet))
                failures.append(full_path)
            thread.start()
            threads.append((thread,file))
    for thread,name in threads:
        thread.join()
        if not quiet:
            print(f'[RUN_PROCESS]: Generating Tags on {name} finished')
    # then spread out no-returning threads for OCR & chroma db, and make logs
    # for file in failures:
    #     print(f'[RUN_PROCESS]: OCR on {file} beginning....')
    #     thread = threading.Thread(target=make_db,args=(file,ocr_log_dir,quiet))
    #     thread.start()

if __name__ == '__main__':
    run(root_dir='./tests/data/simple_pptx',quiet=False)
